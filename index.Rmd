---
title       : Opportunity Index
subtitle    : Updating CT Opportunity Index data
author      : Scott Gaul
job         : Community Indicators Project
framework   : minimal        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
ext_widgets : {rCharts: ["libraries/nvd3"]} 
mode        : selfcontained # {standalone, draft}
markdown    : kramdown
---

## Opportunity Index



### Census data for neighborhoods



```{r echo = FALSE, warning = FALSE, message = FALSE, results='asis'}
library(ggplot2)
library(reshape)
library(acs)
library(maps)
library(maptools)

#Set up Census API key
key = "ba67d3a427e1f785987b9c8bc59341bf7c8a7cc1"
api.key.install(key)

#Load the UConn tract and town-level shapefiles for maps
CTTracts <- readShapeSpatial(fn="tractct_37800_0000_2010_s100_census_1_shp/wgs84/tractct_37800_0000_2010_s100_census_1_shp_wgs84")
CTTracts <- fortify(CTTracts, region = "NAME10")
CTTracts <- CTTracts[order(CTTracts$order),]

#Create tracts for the state
ct.tracts = geo.make(state = "CT", county = "*", tract = "*", check = F)

#Percent of population on public assistance
B19058 = acs.fetch(geography = ct.tracts, table.number = "B19058", col.names = "pretty", endyear = 2012)

B19058.rate = divide.acs(numerator=B19058[,2],denominator=B19058[,1])

B19058.tract = data.frame(geo=geography(B19058)[[1]],
                              publicassistance= 1 - as.numeric(estimate(B19058.rate)))

#Percent of population with college degree including associate degree
B23006 = acs.fetch(geography = ct.tracts, table.number = "B23006", col.names = "pretty", endyear = 2012)

B23006.rate = divide.acs(numerator=(B23006[,16]+B23006[,23]),denominator=B23006[,1])

B23006.tract = data.frame(geo=geography(B23006)[[1]],
                              college = as.numeric(estimate(B23006.rate)))

#Neighborhood poverty rate
B17017 = acs.fetch(geography = ct.tracts, table.number = "B17017", col.names = "pretty", endyear = 2012)

B17017.rate = divide.acs(numerator=B17017[,2], denominator=B17017[,1])

B17017.tract = data.frame(geo=geography(B17017)[[1]], 
                     poverty= 1 - as.numeric(estimate(B17017.rate)))

#Unemployment rate
B23025 = acs.fetch(geography = ct.tracts, table.number = "B23025", col.names = "pretty")

B23025.rate = divide.acs(numerator=B23025[,5],denominator=B23025[,2])

B23025.tract = data.frame(geo=geography(B23025)[,1],
                              unemployment= 1 - as.numeric(estimate(B23025.rate)))

#Home ownership rate - % of owner-occupied homes in housing stock
B25008 = acs.fetch(geography = ct.tracts, table.number = "B25008", col.names = "pretty", endyear = 2012)

B25008.rate = divide.acs(numerator=B25008[,2],denominator=B25008[,1])

B25008.tract = data.frame(geo=geography(B25008)[[1]],
                          owneroccupied= as.numeric(estimate(B25008.rate)))

#Mean commute time, method here: http://quickfacts.census.gov/qfd/meta/long_LFE305212.htm
B08013 = acs.fetch(geography = ct.tracts, table.number = "B08013", col.names = "pretty", endyear = 2012)
#B08135 = acs.fetch(geography = ct.tracts, table.number = "B08135", col.names = "pretty", endyear = 2012)
B99084 = acs.fetch(geography = ct.tracts, table.number = "B99084", col.names = "pretty", endyear = 2012)

B08013.rate = divide.acs(numerator=B08013[,1],denominator=B99084[,2])

B08013.tract = data.frame(geo=geography(B08013)[[1]],
                          commutetime= (-1) * as.numeric(estimate(B08013.rate)))

#Neighborhood vacancy rate
B25002 = acs.fetch(geography = ct.tracts, table.number = "B25002", col.names = "pretty", endyear = 2012)

B25002.rate = divide.acs(numerator=B25002[,3],denominator=B25002[,1])

B25002.tract = data.frame(geo=geography(B25002)[[1]],
                          vacancy= 1 - as.numeric(estimate(B25002.rate)))

oppdata <- data.frame(B23006.tract,
                      B19058.tract[2], 
                      B17017.tract[2],
                      B23025.tract[2],
                      B25008.tract[2],
                      B08013.tract[2],
                      B25002.tract[2],
                      check.names = F)

oppdata$geo= gsub("Census Tract ", "", oppdata$geo)
oppdata$geo= gsub(", (Fairfield|Hartford|Litchfield|Middlesex|New Haven|New London|Tolland|Windham) County, Connecticut","", oppdata$geo)

library(xtable)
print(xtable(summary(oppdata[2:8])), type = "html")
```

### Town data for jobs and test scores

 

```{r echo = FALSE, warning = FALSE, message = FALSE, results = 'asis'}
#Load the tract mapping from UConn
library(reshape)
mapping <- read.csv('tractstowns2.csv')

#Add the town names to the tract-level data
oppdata <- merge(oppdata, mapping[c("NAME10","NAME10_1")], by.x = "geo", by.y = "NAME10", all.x = T)

#Load the qcew data
qcew <- read.csv('qcew-annual-averages.csv', na.strings = "*")

#Cast into new format to get growth rate in employment
qcew_t <- cast(qcew, Town ~ Year, value = "Annual.Average.Employment")
names(qcew_t) <- make.names(names(qcew_t))
qcew_t$jobchange = (qcew_t$X2012 - qcew_t$X2009) / as.numeric(qcew_t$X2009)

oppdata <- merge(oppdata, 
                 qcew_t[c("Town","jobchange")], 
                 by.x = "NAME10_1", 
                 by.y = "Town", 
                 all.x = T)

#Load the test scores data
#Need to manually change code regional school districts
#Fill missing years for some towns, i.e. Newtown 2013
cmt <- read.csv('ctreports-2013-grade-3-cmt.csv', na.strings = "-")

#Merge avg. scale scores with rest of oppdata
oppdata <- merge(oppdata, 
                 cmt[c("Group",
                       "Total.Mathematics.Avg.Scale.Score",
                       "Total.Reading.Average.Scale.Score")], 
                 by.x = "NAME10_1", 
                 by.y = "Group", 
                 all.x = T)

library(xtable)
print(xtable(summary(oppdata[10:12])), type = "html")
```

### Calculating z-scores for the index



```{r echo = FALSE, warning = FALSE, message = FALSE}
ggplot(data = melt(oppdata)) + 
         geom_density(aes(x = value)) + 
         facet_wrap(~ variable, ncol = 2, scales = "free")
```

If the data are not normally distributed - if, for instance, they are skewed or there are multiple modes in the data - then the z-scores can be harder to interpret. And it's also harder to compare the z-scores across variables - a z-score of 2 for poverty doesn't mean the same thing as a z-score of 2 for commute time if they don't have the same-shaped distribution. 

This matters since the opportunity index is calculated using the average z-scores across all of the variables. If the variables have different distributions, then the z-scores will have different ranges and the z-scores won't have the same interpretation or influence on the final index values. 

The charts below show the standardized results for each variable - they report the z-scores between -/+3 standard deviations for each variable. You can see that variables like poverty, public assistance, unemployment tend to have similar shapes and are skewed positive - there are many above-average tracts, but a long tail of tracts with below-average scores on these variables. 

```{r echo = FALSE, warning = FALSE, message = FALSE}
oppdata[3:12] <- scale(oppdata[3:12], center = T, scale = T)

ggplot(data = melt(oppdata)) +
  geom_density(aes(x = value)) +
  xlim(-3,3) +
  facet_wrap(~ variable, ncol = 2, scales = "free_y")
```

To see this visually, we can map each of the variables for the state. Several variables - like poverty, public assistance, unemployment - show similar patterns across tracts, while job growth and vacancy are less similar. 

```{r echo = FALSE, warning = FALSE, message = FALSE, fig.height = 15, fig.width = 12}
#Merge with data
library(classInt)
choropleth=merge(CTTracts, melt(oppdata), by.x = "id", by.y="geo")
choropleth=choropleth[order(choropleth$order), ]
breaks <- classIntervals(melt(oppdata)$value, n=5, style="fisher")
choropleth$value=cut(choropleth$value, 
                      breaks=breaks$brks,
                      include.lowest=T, dig.lab = T)
#Make the map
ggplot(data = choropleth, aes(long, lat, group = group)) +
  geom_polygon(aes(fill = value)) + 
  scale_x_continuous(breaks = NULL) +
  scale_y_continuous(breaks = NULL) +
  labs(x = NULL, y = NULL) + 
  coord_equal() +
#  geom_polygon(data = CTTracts, colour = "grey", alpha = 0.1, fill = NA) +
  scale_fill_brewer(palette = "Purples", name = "Z-scores") +
  theme_minimal() + 
  facet_wrap(~ variable, ncol = 2)
```

The next step is to calculate the opportunity index from the z-scores of the individual variables. Even this has some choices involved - the Kirwan Institute mapping uses [quintiles to color the maps](http://kirwaninstitute.osu.edu/reports/2009/11_2009_CTOppMapping_FullReport.pdf), which means 1/5th of the tracts will fall into each color category. The map below shows the updated index for the state using quintiles. 

```{r echo = FALSE, warning = FALSE, message = FALSE, fig.height = 6, fig.width = 12}
#Merge with data
oppdata$index = rowMeans(oppdata[3:12], na.rm = T)
write.csv(oppdata, "oppdata.csv", row.names = F)

library(classInt)
choropleth=merge(CTTracts, oppdata[c("geo","index")], 
                 by.x = "id", by.y="geo")
choropleth=choropleth[order(choropleth$order), ]
breaks <- classIntervals(oppdata$index, n=5, style="quantile")
choropleth$index=cut(choropleth$index, 
                      breaks=breaks$brks,
                      include.lowest=T, dig.lab = T)
#Make the map
ggplot(data = choropleth, aes(long, lat, group = group)) +
  geom_polygon(aes(fill = index)) + 
  scale_x_continuous(breaks = NULL) +
  scale_y_continuous(breaks = NULL) +
  labs(x = NULL, y = NULL) + 
  coord_equal() +
#  geom_polygon(data = CTTracts, colour = "grey", alpha = 0.1, fill = NA) +
  scale_fill_brewer(palette = "Purples", name = "Opportunty index\n(Z-scores - quintiles)") +
  theme_minimal(base_size = 14)
```



```{r echo = FALSE, warning = FALSE, message = FALSE, fig.height = 6, fig.width = 12}
#Merge with data
choropleth=merge(CTTracts, oppdata[c("geo","index")], 
                 by.x = "id", by.y="geo")
choropleth=choropleth[order(choropleth$order), ]
breaks <- classIntervals(oppdata$index, n=5, style="jenks")
choropleth$index=cut(choropleth$index, 
                      breaks=breaks$brks,
                      include.lowest=T, dig.lab = T)
#Make the map
ggplot(data = choropleth, aes(long, lat, group = group)) +
  geom_polygon(aes(fill = index)) + 
  scale_x_continuous(breaks = NULL) +
  scale_y_continuous(breaks = NULL) +
  labs(x = NULL, y = NULL) + 
  coord_equal() +
#  geom_polygon(data = CTTracts, colour = "grey", alpha = 0.1, fill = NA) +
  scale_fill_brewer(palette = "Purples", name = "Opportunity index\n(Z-scores, Jenks)") +
  theme_minimal(base_size = 14)
```

This map reflects fewer areas of 'very low' opportunity, but more areas of 'low' and 'moderate' opportunity. Another way to see this is to plot the distribution of the index values for the tracts, including the breakpoints. The chart below shows the breakpoints using the quintiles. (Again, the overall distribution is skewed positive.)

```{r echo = FALSE, warning = FALSE, message = FALSE, fig.height = 3}
breaks <- classIntervals(oppdata$index, n=5, style="quantile")
ggplot(data = oppdata) +
  geom_density(aes(x = index)) + 
   xlim(-4,4) +
  geom_vline(xintercept = breaks$brks)
```

And this chart shows the breakpoints using the natural breaks method. 

```{r echo = FALSE, warning = FALSE, message = FALSE, fig.height = 3}
breaks <- classIntervals(oppdata$index, n=5, style="jenks")
ggplot(data = oppdata) +
  geom_density(aes(x = index)) + 
    xlim(-4,4) +
  geom_vline(xintercept = breaks$brks)
```

Using quintiles means that roughly 20 percent of the population will always live in high opportunity areas (since census tracts have roughly similar population), while the Jenks breaks (or other methods) would reflect the concentration of poverty in a smaller set of areas. 

### What is driving the Opportunity Index? 

With a composite index of z-scores, it helps to see if specific variables are playing more of a role in determining the final index values. The [OECD guide to composite indicators](http://www.oecd.org/std/42495745.pdf) notes that using z-scores means that 'indicators with extreme values thus have a greater effect on the composite indicator.' That can be an issue in a state with a high degree of inequality and concentration of poverty. 

As a start, we know that many of the variables are correlated with each other - the correlation matrix below shows that several of the variables - poverty, public assistance, etc. - are correlated with each other. Job growth has almost no correlation with any of the variables. 

```{r echo = FALSE, warning = FALSE, message = FALSE, results = 'asis'}
x <- subset(oppdata[3:12], commutetime != "NA" & 
              Total.Mathematics.Avg.Scale.Score != "NA" &
              Total.Reading.Average.Scale.Score != "NA" & 
              jobchange != "NA")
print(xtable(round(cor(x),2)), type = "html")
```

A scatterplot matrix shows the same visually - job growth and (to a lesser extent) commute time have little obvious relationship with the other variables. 

```{r echo = FALSE, warning = FALSE, message = FALSE}
plot(oppdata[3:12])
```

A principal components analysis shows that the first principal component dominates the results - explaining 56 percent of the overall variance in the index  (first bar in the graph, first column in the table). 

```{r echo = FALSE, warning = FALSE, message = FALSE, results = 'asis'}
pca1 = prcomp(x, scale. = T)
print(xtable(summary(pca1)), type = "html")
plot(prcomp(x, scale = T))
#print(pca1$rotation[,1])
```

We can look at the weights for each of the variables in the first principal component in the chart below. This shows that job growth has little influence on the first component (weight close to 0), while commute time offsets some of the other variables (positive weight). Poverty, public assistance and owner-occupied housing have the strongest weights. 

```{r echo = FALSE, warning = FALSE, message = FALSE, fig.height = 4}
rotation <- data.frame(pca1$rotation[,1])
qplot(data = rotation, x = pca1.rotation...1., y = row.names(rotation)) + 
  labs(x = "Weight in first principal component", y = NULL)
```

In other words, since much of the variance is explained by the first principal components and since poverty, public assistance, owner-occupied housing, educational attainment, test scores and unemployment have the most weight for that component, most of the opportunity index is described by these variables (poverty, public assistance, etc.). 

Not surprisingly, many of these variables also have skewed distributions for Connecticut, and hence a more extreme range of z-scores to factor into the overall index.

Another way to look at this is to see how well these variables predict the final index values. For example, poverty alone predicts the overall index pretty well - the R-squared is 0.78 - meaning that the variation in poverty alone explains 78% of the variation in opportunity. 

```{r echo = FALSE, warning = FALSE, message = FALSE, fig.height = 4}
ggplot(data = subset(oppdata, poverty > -6), aes(y = index, x = poverty)) + 
  geom_point() + 
  geom_smooth(method = lm) + 
  labs(y = "Opportunity Index", x = "Poverty z-scores")
#summary(lm(index ~ publicassistance, data = oppdata))
```

Overall: 
* We can re-calculate the opportunity index using Census data and state data for towns on test scores and jobs, making some concessions for data availability on the latter variables.
* Timing is a consideration - the Census data covers a different time horizon than the other variables. (But the original index used 2000 census data and jobs data for 2005 - 2008, so this may be less of a concern.)
* The index is driven largely by poverty and variables like public assistance that are strongly correlated with poverty. 
* Different ways to display the data will yield different conclusions about the landscape of opportunity in Connecticut. 
* Job growth (economic climate) has the least influence on the index as it is uncorrelated with the other variables. 

